## 定义
用来产生词向量的相关模型.

## 简介

为了使计算机能够处理自然语言，首先需要对自然语言进行建模。

自然语言建模方法经历了从基于规则的方法到基于统计方法的转变。从基于统计的建模方法得到的自然语言模型称为统计语言模型。有许多统计语言建模技术，包括n－gram、神经网络以及 log_linear 模型等。

在对自然语言进行建模的过程中，会出现 **维数灾难**、**词语相似性**、**模型泛化能力**以及 **模型性能**等问题。

寻找上述问题的解决方案是推动统计语言模型不断发展的内在动力。在对统计语言模型进行研究的背景下，Google 公司在 2013年开放了 Word2vec这一款用于训练词向量的软件工具。

Word2vec 可以根据给定的语料库，通过优化后的训练模型快速有效地将一个词语表达成向量形式，为自然语言处理领域的应用研究提供了新的工具。Word2vec依赖skip-grams或连续词袋（CBOW）来建立神经词嵌入。Word2vec为托马斯·米科洛夫（Tomas Mikolov）在Google带领的研究团队创造。该算法渐渐被其他人所分析和解释。

## 相关概念
- 词袋模型(Bag-of-words model)

此模型下，像是句子或是文件这样的文字可以用一个袋子装着这些词的方式表现，这种表现方式不考虑文法以及词的顺序。

- Skip-gram 模型

在自然语言处理中，语料的选取是一个相当重要的问题: 第一，**语料必须充分**。一方面词典的词量要足够大，另一方面要尽可能多地包含反映词语之间关系的句子，例如，只有“鱼在水中游”这种句式在语料中尽可能地多，模型才能够学习到该句中的语义和语法关系，这和人类学习自然语言一个道理，重复的次数多了，也就会模仿了； 第二，**语料必须准确**。也就是说所选取的语料能够正确反映该语言的语义和语法关系，这一点似乎不难做到，例如中文里，《人民日报》的语料比较准确。 但是，更多的时候，并不是语料的选取引发了对准确性问题的担忧，而是处理的方法。 **n元模型中，因为窗口大小的限制，导致超出窗口范围的词语与当前词之间的关系不能被正确地反映到模型之中，如果单纯扩大窗口大小又会增加训练的复杂度。**

Skip-gram 模型的提出很好地解决了这些问题。

顾名思义，Skip-gram 就是“跳过某些符号”，例如，句子“中国足球踢得真是太烂了”有4个3元词组，分别是“中国足球踢得”、“足球踢得真是”、“踢得真是太烂”、“真是太烂了”，可是我们发现，这个句子的本意就是“中国足球太烂”可是上述 4个3元词组并不能反映出这个信息。Skip-gram 模型却允许某些词被跳过，因此可以组成“中国足球太烂”这个3元词组。 如果允许跳过2个词，即 2-Skip-gram。


[](https://blog.csdn.net/mylove0414/article/details/61616617)


word2vec采用的是n元语法模型(n-gram model)，即假设一个词只与周围n个词有关，而与文本中的其他词无关。这种模型构建简单直接，当然也有后续的各种平滑方法[2]，这里就不展开了。

https://blog.csdn.net/u014595019/article/details/51884529

# 模型拆解
word2vec模型其实就是简单化的神经网络。

输入是One-Hot Vector，Hidden Layer没有激活函数，也就是线性的单元。Output Layer维度跟Input Layer的维度一样，用的是Softmax回归。我们要获取的dense vector其实就是Hidden Layer的输出单元。有的地方定为Input Layer和Hidden Layer之间的权重，其实说的是一回事。


## 基于Hierarchical Softmax的模型
(1) n : 一个词的上下文包含的词数，与n-gram中n的含义相同 
(2) m : 词向量的长度，通常在10~100 
(3) h : 隐藏层的规模，一般在100量级 
(4) N ：词典的规模，通常在1W~10W 
(5) T : 训练文本中单词个数



